{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Server Load Balancing - Pipeline Test\n",
    "\n",
    "This notebook demonstrates the Server Load (M/M/k queueing) environment with two agents:\n",
    "- **LinUCB**: Contextual bandit with UCB exploration\n",
    "- **DQN**: Deep Q-Network with experience replay\n",
    "\n",
    "We compare their learning curves and performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our modules\n",
    "from src.envs import ServerLoadEnv, ServerLoadConfig\n",
    "from src.agents import LinUCBAgent, DQNAgent\n",
    "from src.utils.seeding import set_global_seeds\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Configure the M/M/k queueing environment with:\n",
    "- 4 servers\n",
    "- Arrival rate λ = 10 jobs/sec\n",
    "- Service rate μ = 3 jobs/sec per server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment configuration\n",
    "config = ServerLoadConfig(\n",
    "    num_servers=4,\n",
    "    arrival_rate=10.0,\n",
    "    service_rate=3.0,\n",
    "    max_queue_size=50,\n",
    "    step_duration=1.0,\n",
    "    alpha=1.0,   # Latency penalty\n",
    "    beta=10.0,   # Drop penalty\n",
    "    gamma=0.1,   # Server cost\n",
    ")\n",
    "\n",
    "# Create environment\n",
    "env = ServerLoadEnv(config=config, seed=42)\n",
    "state = env.reset()\n",
    "\n",
    "print(f\"State dimension: {len(state)}\")\n",
    "print(f\"Action space: {env.action_space.n} servers\")\n",
    "print(f\"\\nInitial state: {state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize environment\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training Function\n",
    "\n",
    "Generic training loop that works with any agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent, env, n_episodes=200, max_steps=100, seed=42):\n",
    "    \"\"\"\n",
    "    Train an agent on the environment.\n",
    "    \n",
    "    Returns:\n",
    "        dict with training history\n",
    "    \"\"\"\n",
    "    set_global_seeds(seed)\n",
    "    env.reset(seed=seed)\n",
    "    \n",
    "    history = {\n",
    "        'episode_rewards': [],\n",
    "        'episode_lengths': [],\n",
    "        'avg_latencies': [],\n",
    "        'total_drops': [],\n",
    "        'queue_lengths': [],\n",
    "    }\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_latencies = []\n",
    "        episode_queue_lengths = []\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Select action\n",
    "            action = agent.select_action(state)\n",
    "            \n",
    "            # Take step\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Store transition\n",
    "            agent.store(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Train if ready\n",
    "            if agent.ready_to_train():\n",
    "                agent.update()\n",
    "            \n",
    "            # Track metrics\n",
    "            episode_reward += reward\n",
    "            if info.get('avg_latency', 0) > 0:\n",
    "                episode_latencies.append(info['avg_latency'])\n",
    "            episode_queue_lengths.append(np.mean(info['queue_lengths']))\n",
    "            \n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Record episode stats\n",
    "        history['episode_rewards'].append(episode_reward)\n",
    "        history['episode_lengths'].append(step + 1)\n",
    "        history['avg_latencies'].append(\n",
    "            np.mean(episode_latencies) if episode_latencies else 0\n",
    "        )\n",
    "        history['total_drops'].append(env.total_drops)\n",
    "        history['queue_lengths'].append(np.mean(episode_queue_lengths))\n",
    "        \n",
    "        # Progress update\n",
    "        if (episode + 1) % 50 == 0:\n",
    "            avg_r = np.mean(history['episode_rewards'][-50:])\n",
    "            print(f\"Episode {episode+1:4d} | Avg Reward: {avg_r:8.2f}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Baseline\n",
    "\n",
    "First, establish a random baseline for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    \"\"\"Random action baseline agent.\"\"\"\n",
    "    def __init__(self, n_actions):\n",
    "        self.n_actions = n_actions\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        return np.random.randint(0, self.n_actions)\n",
    "    \n",
    "    def store(self, *args):\n",
    "        pass\n",
    "    \n",
    "    def ready_to_train(self):\n",
    "        return False\n",
    "    \n",
    "    def update(self):\n",
    "        return {}\n",
    "\n",
    "# Train random baseline\n",
    "print(\"Training Random Baseline...\")\n",
    "random_agent = RandomAgent(n_actions=4)\n",
    "random_history = train_agent(random_agent, env, n_episodes=200, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LinUCB Agent\n",
    "\n",
    "Contextual bandit with Upper Confidence Bound exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LinUCB agent\n",
    "state_dim = len(env.reset())\n",
    "linucb_agent = LinUCBAgent(\n",
    "    n_arms=4,\n",
    "    context_dim=state_dim,\n",
    "    alpha=1.5,  # UCB exploration parameter\n",
    "    regularization=1.0,\n",
    ")\n",
    "\n",
    "print(\"Training LinUCB Agent...\")\n",
    "linucb_history = train_agent(linucb_agent, env, n_episodes=200, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. DQN Agent\n",
    "\n",
    "Deep Q-Network with experience replay and target network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DQN agent\n",
    "dqn_agent = DQNAgent(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=4,\n",
    "    hidden_dims=(128, 128),\n",
    "    gamma=0.99,\n",
    "    lr=1e-3,\n",
    "    buffer_size=10000,\n",
    "    batch_size=64,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.05,\n",
    "    epsilon_decay=0.995,\n",
    "    target_update_freq=50,\n",
    "    min_buffer_size=500,\n",
    "    double_dqn=True,\n",
    ")\n",
    "\n",
    "print(\"Training DQN Agent...\")\n",
    "dqn_history = train_agent(dqn_agent, env, n_episodes=200, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Learning Curves Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(data, window=10):\n",
    "    \"\"\"Compute moving average.\"\"\"\n",
    "    kernel = np.ones(window) / window\n",
    "    return np.convolve(data, kernel, mode='valid')\n",
    "\n",
    "# Plot learning curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Episode Rewards\n",
    "ax = axes[0, 0]\n",
    "ax.plot(smooth(random_history['episode_rewards'], 10), label='Random', alpha=0.7, linestyle='--')\n",
    "ax.plot(smooth(linucb_history['episode_rewards'], 10), label='LinUCB', alpha=0.9)\n",
    "ax.plot(smooth(dqn_history['episode_rewards'], 10), label='DQN', alpha=0.9)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Episode Reward (smoothed)')\n",
    "ax.set_title('Learning Curves: Episode Reward')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Average Latency\n",
    "ax = axes[0, 1]\n",
    "ax.plot(smooth(random_history['avg_latencies'], 10), label='Random', alpha=0.7, linestyle='--')\n",
    "ax.plot(smooth(linucb_history['avg_latencies'], 10), label='LinUCB', alpha=0.9)\n",
    "ax.plot(smooth(dqn_history['avg_latencies'], 10), label='DQN', alpha=0.9)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Average Latency (seconds)')\n",
    "ax.set_title('Service Latency Over Training')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Queue Lengths\n",
    "ax = axes[1, 0]\n",
    "ax.plot(smooth(random_history['queue_lengths'], 10), label='Random', alpha=0.7, linestyle='--')\n",
    "ax.plot(smooth(linucb_history['queue_lengths'], 10), label='LinUCB', alpha=0.9)\n",
    "ax.plot(smooth(dqn_history['queue_lengths'], 10), label='DQN', alpha=0.9)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Avg Queue Length')\n",
    "ax.set_title('Queue Management')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative Drops\n",
    "ax = axes[1, 1]\n",
    "ax.plot(random_history['total_drops'], label='Random', alpha=0.7, linestyle='--')\n",
    "ax.plot(linucb_history['total_drops'], label='LinUCB', alpha=0.9)\n",
    "ax.plot(dqn_history['total_drops'], label='DQN', alpha=0.9)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Total Dropped Jobs')\n",
    "ax.set_title('Cumulative Job Drops (Economic Loss)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../logs/learning_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stats(history, last_n=50):\n",
    "    \"\"\"Compute summary statistics from last N episodes.\"\"\"\n",
    "    return {\n",
    "        'avg_reward': np.mean(history['episode_rewards'][-last_n:]),\n",
    "        'std_reward': np.std(history['episode_rewards'][-last_n:]),\n",
    "        'avg_latency': np.mean(history['avg_latencies'][-last_n:]),\n",
    "        'avg_queue': np.mean(history['queue_lengths'][-last_n:]),\n",
    "        'total_drops': history['total_drops'][-1],\n",
    "    }\n",
    "\n",
    "random_stats = compute_stats(random_history)\n",
    "linucb_stats = compute_stats(linucb_history)\n",
    "dqn_stats = compute_stats(dqn_history)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PERFORMANCE SUMMARY (Last 50 Episodes)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Metric':<20} {'Random':>15} {'LinUCB':>15} {'DQN':>15}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Avg Reward':<20} {random_stats['avg_reward']:>15.2f} {linucb_stats['avg_reward']:>15.2f} {dqn_stats['avg_reward']:>15.2f}\")\n",
    "print(f\"{'Reward Std':<20} {random_stats['std_reward']:>15.2f} {linucb_stats['std_reward']:>15.2f} {dqn_stats['std_reward']:>15.2f}\")\n",
    "print(f\"{'Avg Latency (s)':<20} {random_stats['avg_latency']:>15.3f} {linucb_stats['avg_latency']:>15.3f} {dqn_stats['avg_latency']:>15.3f}\")\n",
    "print(f\"{'Avg Queue Length':<20} {random_stats['avg_queue']:>15.2f} {linucb_stats['avg_queue']:>15.2f} {dqn_stats['avg_queue']:>15.2f}\")\n",
    "print(f\"{'Total Drops':<20} {random_stats['total_drops']:>15d} {linucb_stats['total_drops']:>15d} {dqn_stats['total_drops']:>15d}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Economic Loss Analysis\n",
    "\n",
    "Calculate the economic impact of job drops and latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Economic parameters\n",
    "COST_PER_DROP = 10.0      # $ per dropped job\n",
    "COST_PER_LATENCY_SEC = 1.0  # $ per second of latency\n",
    "\n",
    "def compute_economic_loss(history, config):\n",
    "    \"\"\"Compute total economic loss.\"\"\"\n",
    "    drop_cost = history['total_drops'][-1] * COST_PER_DROP\n",
    "    latency_cost = sum(history['avg_latencies']) * COST_PER_LATENCY_SEC\n",
    "    return drop_cost, latency_cost, drop_cost + latency_cost\n",
    "\n",
    "# Compute losses\n",
    "random_loss = compute_economic_loss(random_history, config)\n",
    "linucb_loss = compute_economic_loss(linucb_history, config)\n",
    "dqn_loss = compute_economic_loss(dqn_history, config)\n",
    "\n",
    "# Plot economic losses\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "agents = ['Random', 'LinUCB', 'DQN']\n",
    "drop_costs = [random_loss[0], linucb_loss[0], dqn_loss[0]]\n",
    "latency_costs = [random_loss[1], linucb_loss[1], dqn_loss[1]]\n",
    "\n",
    "x = np.arange(len(agents))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, drop_costs, width, label='Drop Cost', color='#e74c3c')\n",
    "bars2 = ax.bar(x + width/2, latency_costs, width, label='Latency Cost', color='#3498db')\n",
    "\n",
    "ax.set_xlabel('Agent')\n",
    "ax.set_ylabel('Economic Loss ($)')\n",
    "ax.set_title('Economic Loss Breakdown by Agent')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(agents)\n",
    "ax.legend()\n",
    "\n",
    "# Add total labels\n",
    "for i, (d, l) in enumerate(zip(drop_costs, latency_costs)):\n",
    "    ax.annotate(f'Total: ${d+l:.0f}', xy=(i, max(d, l)), \n",
    "                xytext=(0, 10), textcoords='offset points',\n",
    "                ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../logs/economic_loss.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nEconomic Loss Summary:\")\n",
    "print(f\"  Random: ${random_loss[2]:.2f}\")\n",
    "print(f\"  LinUCB: ${linucb_loss[2]:.2f} (savings: ${random_loss[2] - linucb_loss[2]:.2f})\")\n",
    "print(f\"  DQN:    ${dqn_loss[2]:.2f} (savings: ${random_loss[2] - dqn_loss[2]:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Agent Behavior Visualization\n",
    "\n",
    "Visualize how each agent distributes load across servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_routing(agent, env, n_steps=500, seed=42):\n",
    "    \"\"\"Analyze agent routing behavior.\"\"\"\n",
    "    env.reset(seed=seed)\n",
    "    actions = []\n",
    "    states = []\n",
    "    \n",
    "    state = env.reset()\n",
    "    for _ in range(n_steps):\n",
    "        action = agent.select_action(state, explore=False)  # Greedy\n",
    "        actions.append(action)\n",
    "        states.append(state.copy())\n",
    "        state, _, done, _ = env.step(action)\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "    \n",
    "    return np.array(actions), np.array(states)\n",
    "\n",
    "# Analyze routing for each agent\n",
    "random_actions, _ = analyze_routing(random_agent, env)\n",
    "linucb_actions, linucb_states = analyze_routing(linucb_agent, env)\n",
    "dqn_actions, dqn_states = analyze_routing(dqn_agent, env)\n",
    "\n",
    "# Plot action distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "for ax, actions, title in zip(axes, \n",
    "                               [random_actions, linucb_actions, dqn_actions],\n",
    "                               ['Random', 'LinUCB', 'DQN']):\n",
    "    counts = np.bincount(actions, minlength=4)\n",
    "    ax.bar(range(4), counts, color=['#3498db', '#2ecc71', '#e74c3c', '#9b59b6'])\n",
    "    ax.set_xlabel('Server ID')\n",
    "    ax.set_ylabel('Times Selected')\n",
    "    ax.set_title(f'{title} - Routing Distribution')\n",
    "    ax.set_xticks(range(4))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../logs/routing_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Learning Progress**: Both LinUCB and DQN show improvement over the random baseline.\n",
    "\n",
    "2. **LinUCB**: Quick adaptation due to online learning, good for simpler routing patterns.\n",
    "\n",
    "3. **DQN**: Better long-term performance due to temporal credit assignment, but requires more samples.\n",
    "\n",
    "4. **Economic Impact**: Trained agents significantly reduce job drops and latency costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Notebook execution complete!\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
